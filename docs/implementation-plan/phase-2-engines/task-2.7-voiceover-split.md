# Task 2.7: é…éŸ³åˆ‡åˆ†

## ğŸ“‹ Task è³‡è¨Š

| é …ç›® | å…§å®¹ |
|------|------|
| **Task ID** | 2.7 |
| **Task åç¨±** | é…éŸ³åˆ‡åˆ† |
| **æ‰€å±¬ Phase** | Phase 2: æ ¸å¿ƒå¼•æ“é–‹ç™¼ |
| **é ä¼°æ™‚é–“** | 2-3 å°æ™‚ (STT æ•´åˆ 1h + åˆ‡åˆ†å¯¦ä½œ 1h + æ¸¬è©¦ 1h) |
| **é›£åº¦** | â­â­ ä¸­ç­‰ |
| **å‰ç½® Task** | Task 2.5 (Whisper STT æ•´åˆ) |

## ğŸ†˜ é‡åˆ°å•é¡Œæ€éº¼è¾¦?

### Step 1: å…ˆçœ‹éŒ¯èª¤è¨Šæ¯

**å¸¸è¦‹çš„é…éŸ³åˆ‡åˆ†å•é¡Œ**:

1. **æ‰¾åˆ°éŒ¯èª¤çš„é—œéµå­—**
   ```
   Error: Audio codec not supported
          ^^^^^^^^^^^^^^^^^^^^^^^  â† éŸ³è¨Šæ ¼å¼ä¸æ”¯æ´
   ```

2. **åˆ¤æ–·éŒ¯èª¤é¡å‹**
   - `Audio codec not supported` â†’ éŸ³è¨Šæ ¼å¼å•é¡Œ
   - `Invalid timestamp` â†’ æ™‚é–“ç¢¼æ ¼å¼éŒ¯èª¤
   - `Segment duration too short` â†’ ç‰‡æ®µå¤ªçŸ­
   - `FFmpeg command failed` â†’ FFmpeg åŸ·è¡Œå¤±æ•—

---

### Step 2: ä¸Šç¶²æœå°‹

#### ğŸ” æœå°‹æŠ€å·§

**âŒ ä¸å¥½çš„æœå°‹æ–¹å¼**:
```
"é…éŸ³åˆ‡ä¸äº†"  â† å¤ªæ¨¡ç³Š
"éŸ³è¨Šè™•ç†éŒ¯èª¤" â† æ²’æœ‰å…·é«”è³‡è¨Š
```

**âœ… å¥½çš„æœå°‹æ–¹å¼**:
```
"FFmpeg split audio by timestamp"  â† å…·é«”åŠŸèƒ½
"Whisper API word-level timestamps" â† æ˜ç¢ºçš„æ™‚é–“ç¢¼å•é¡Œ
"FFmpeg preserve audio quality" â† å“è³ªç›¸é—œ
```

#### ğŸŒ æ¨è–¦è³‡æº

**å„ªå…ˆé †åº 1: å®˜æ–¹æ–‡ä»¶** (æœ€æº–ç¢º)
- FFmpeg: https://ffmpeg.org/documentation.html
- Whisper API: https://platform.openai.com/docs/guides/speech-to-text

**å„ªå…ˆé †åº 2: éŸ³è¨Šè™•ç†æŒ‡å—**
- Audio Processing: https://trac.ffmpeg.org/wiki/AudioChannelManipulation

---

### Step 3: æª¢æŸ¥ FFmpeg èˆ‡ Whisper

```bash
# æª¢æŸ¥ FFmpeg æ˜¯å¦å®‰è£
ffmpeg -version

# æ¸¬è©¦ç°¡å–®çš„éŸ³è¨Šåˆ‡åˆ†
ffmpeg -i input.mp3 -ss 00:00:10 -t 00:00:05 output.mp3

# æª¢æŸ¥ Whisper API é€£æ¥
curl https://api.openai.com/v1/audio/transcriptions \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -F model="whisper-1" \
  -F file="@test.mp3"
```

---

## ğŸ¯ åŠŸèƒ½æè¿°

æ ¹æ“šä½¿ç”¨è€…ä¸Šå‚³çš„é…éŸ³æª”æ¡ˆ,ä½¿ç”¨ Whisper STT å–å¾—é€å­—ç¨¿å’Œæ™‚é–“ç¢¼,ç„¶å¾Œç”¨ FFmpeg æŒ‰ç…§æ™‚é–“ç¢¼åˆ‡åˆ†é…éŸ³æª”,ç”¢ç”Ÿå°æ‡‰çš„é…éŸ³ç‰‡æ®µã€‚

### ç‚ºä»€éº¼éœ€è¦é€™å€‹?

- ğŸ¯ **å•é¡Œ**: ä½¿ç”¨è€…ä¸Šå‚³æ•´æ®µé…éŸ³,éœ€è¦å°æ‡‰åˆ°å½±ç‰‡ç‰‡æ®µæ‰èƒ½è‡ªå‹•é…å°
- âœ… **è§£æ±º**: ä½¿ç”¨ Whisper å–å¾—å­—ç´šæ™‚é–“ç¢¼,æŒ‰ç…§å­—å¹•åˆ‡åˆ†é…éŸ³æª”
- ğŸ’¡ **æ¯”å–»**: å°±åƒæŠŠé•·ç¯‡æ¼”è¬›åˆ‡æˆä¸€å¥ä¸€å¥,æ¯å¥è©±éƒ½æœ‰å°æ‡‰çš„éŸ³æª”å’Œæ™‚é–“

### å®Œæˆå¾Œä½ æœƒæœ‰:

- âœ… Whisper STT æ•´åˆå®Œæˆ
- âœ… è‡ªå‹•å–å¾—é…éŸ³çš„é€å­—ç¨¿å’Œæ™‚é–“ç¢¼
- âœ… æŒ‰ç…§å­—å¹•æ™‚é–“è‡ªå‹•åˆ‡åˆ†é…éŸ³æª”
- âœ… æ¯å€‹é…éŸ³ç‰‡æ®µéƒ½æœ‰å°æ‡‰çš„éŸ³æª”å’Œæ–‡å­—
- âœ… é…éŸ³ç‰‡æ®µå„²å­˜åˆ°è³‡æ–™åº«å’Œå„²å­˜ç©ºé–“
- âœ… æ”¯æ´å¤šç¨®éŸ³è¨Šæ ¼å¼

---

## ğŸ“š å‰ç½®çŸ¥è­˜

ä»¥ä¸‹æ˜¯é€™å€‹ Task æœƒç”¨åˆ°çš„æŠ€è¡“ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰ä¹Ÿæ²’é—œä¿‚,åªè¦ç…§è‘—æ­¥é©Ÿåšå°±èƒ½å®Œæˆã€‚

### 1. OpenAI Whisper STT

**æ˜¯ä»€éº¼**: OpenAI çš„èªéŸ³è½‰æ–‡å­— API,æ”¯æ´å¤šèªè¨€ä¸”æº–ç¢ºåº¦é«˜

**æ ¸å¿ƒæ¦‚å¿µ**:
- **Word-level Timestamps**: å¯ä»¥å–å¾—æ¯å€‹å­—çš„é–‹å§‹å’ŒçµæŸæ™‚é–“
- **èªè¨€åµæ¸¬**: è‡ªå‹•åµæ¸¬èªè¨€
- **æ¨™é»ç¬¦è™Ÿ**: è‡ªå‹•åŠ å…¥æ¨™é»ç¬¦è™Ÿ
- **å¤šæ ¼å¼æ”¯æ´**: æ”¯æ´ mp3, mp4, m4a, wav ç­‰æ ¼å¼

**API å›å‚³ç¯„ä¾‹**:
```json
{
  "task": "transcribe",
  "language": "zh",
  "duration": 10.5,
  "text": "å¤§å®¶å¥½,ä»Šå¤©è¦ä»‹ç´¹ AI å½±ç‰‡å‰ªè¼¯ã€‚",
  "words": [
    {
      "word": "å¤§å®¶å¥½",
      "start": 0.0,
      "end": 0.8
    },
    {
      "word": "ä»Šå¤©",
      "start": 1.0,
      "end": 1.3
    }
  ]
}
```

**ç‚ºä»€éº¼é¸ Whisper**:
- ç¹é«”ä¸­æ–‡æ”¯æ´è‰¯å¥½
- æº–ç¢ºåº¦é«˜
- åƒ¹æ ¼ä¾¿å®œ ($0.006 / åˆ†é˜)
- å®˜æ–¹ API ç©©å®š

### 2. FFmpeg éŸ³è¨Šåˆ‡åˆ†

**æ˜¯ä»€éº¼**: å¼·å¤§çš„éŸ³è¦–è¨Šè™•ç†å·¥å…·

**æ ¸å¿ƒæŒ‡ä»¤**:
```bash
# åŸºæœ¬åˆ‡åˆ†èªæ³•
ffmpeg -i input.mp3 \
  -ss <é–‹å§‹æ™‚é–“> \
  -to <çµæŸæ™‚é–“> \
  -c copy \
  output.mp3

# ç¯„ä¾‹: åˆ‡å‡º 10 ç§’åˆ° 15 ç§’çš„ç‰‡æ®µ
ffmpeg -i voiceover.mp3 \
  -ss 00:00:10 \
  -to 00:00:15 \
  -c copy \
  segment_1.mp3
```

**é‡è¦åƒæ•¸**:
- `-ss`: é–‹å§‹æ™‚é–“ (seek start)
- `-to` æˆ– `-t`: çµæŸæ™‚é–“æˆ–æŒçºŒæ™‚é–“
- `-c copy`: è¤‡è£½ç·¨ç¢¼,ä¸é‡æ–°ç·¨ç¢¼ (å¿«é€Ÿä¸”ä¸å¤±çœŸ)
- `-i`: è¼¸å…¥æª”æ¡ˆ

**ç‚ºä»€éº¼ç”¨ `-c copy`?**
- ä¸é‡æ–°ç·¨ç¢¼,é€Ÿåº¦å¿« 10 å€ä»¥ä¸Š
- ä¸æœƒæå¤±éŸ³è³ª
- ä¸æœƒå¢åŠ æª”æ¡ˆå¤§å°

### 3. æ™‚é–“ç¢¼æ ¼å¼

**å¸¸è¦‹æ ¼å¼**:
```typescript
// ç§’æ•¸æ ¼å¼ (Whisper ä½¿ç”¨)
const timestamp = 12.5; // 12.5 ç§’

// HH:MM:SS.mmm æ ¼å¼ (FFmpeg ä½¿ç”¨)
const timestamp = "00:00:12.500";

// è½‰æ›å‡½å¼
function secondsToFFmpegTime(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = seconds % 60;

  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toFixed(3).padStart(6, '0')}`;
}
```

---

## ğŸ”— å‰ç½®ä¾è³´

### å¿…é ˆå…ˆå®Œæˆçš„ Task
- âœ… Task 2.5: Whisper STT æ•´åˆ (STT æœå‹™å·²å»ºç«‹)
- âœ… Task 1.1: è³‡æ–™åº« Schema (æœ‰ voiceover_segments è¡¨)
- âœ… Task 1.2: Supabase Storage (å¯ä»¥å„²å­˜éŸ³è¨Šæª”)

### API éœ€æ±‚
- OpenAI API Key (ç”¨æ–¼ Whisper)

### ç³»çµ±éœ€æ±‚
- FFmpeg å·²å®‰è£

### å¥—ä»¶ä¾è³´
```json
{
  "dependencies": {
    "openai": "^4.20.0",
    "fluent-ffmpeg": "^2.1.2"
  },
  "devDependencies": {
    "@types/fluent-ffmpeg": "^2.1.24"
  }
}
```

### è³‡æ–™åº« Schema

éœ€è¦ `voiceover_segments` è¡¨:

```sql
CREATE TABLE voiceover_segments (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES auth.users(id),

  -- åŸå§‹é…éŸ³æª”
  original_audio_url TEXT NOT NULL,

  -- ç‰‡æ®µè³‡è¨Š
  text TEXT NOT NULL,
  start_time FLOAT NOT NULL,
  end_time FLOAT NOT NULL,
  duration FLOAT NOT NULL,

  -- åˆ‡åˆ†å¾Œçš„ç‰‡æ®µéŸ³æª”
  segment_audio_url TEXT,

  -- èªæ„åˆ†æ (å¾ŒçºŒä½¿ç”¨)
  keywords JSONB,
  topics JSONB,

  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸ“ å¯¦ä½œæ­¥é©Ÿ

### æ­¥é©Ÿ 1: å®‰è£ç›¸ä¾å¥—ä»¶

```bash
cd backend
npm install openai fluent-ffmpeg
npm install -D @types/fluent-ffmpeg
```

**å¿«é€Ÿé©—è­‰**:
```bash
# æª¢æŸ¥ FFmpeg
ffmpeg -version

# æª¢æŸ¥å¥—ä»¶å®‰è£
node -e "console.log(require('fluent-ffmpeg'))"
```

---

### æ­¥é©Ÿ 2: å»ºç«‹ Whisper æœå‹™ (å¦‚æœå°šæœªå»ºç«‹)

å»ºç«‹ `backend/src/services/whisper.service.ts`:

```typescript
/**
 * Whisper STT æœå‹™
 *
 * ä½¿ç”¨ OpenAI Whisper API é€²è¡ŒèªéŸ³è½‰æ–‡å­—
 */

import OpenAI from 'openai';
import fs from 'fs';
import { logger } from '../lib/logger';
import { CostTrackerService } from './cost-tracker.service';

export class WhisperService {
  private client: OpenAI;
  private costTracker: CostTrackerService;

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY not found in environment variables');
    }

    this.client = new OpenAI({ apiKey });
    this.costTracker = new CostTrackerService();
  }

  /**
   * è½‰éŒ„éŸ³è¨Šæª”æ¡ˆä¸¦å–å¾—å­—ç´šæ™‚é–“ç¢¼
   *
   * @param audioPath - éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
   * @param userId - ä½¿ç”¨è€… ID (ç”¨æ–¼æˆæœ¬è¿½è¹¤)
   * @returns è½‰éŒ„çµæœåŒ…å«æ–‡å­—å’Œæ™‚é–“ç¢¼
   */
  async transcribe(
    audioPath: string,
    userId: string
  ): Promise<WhisperTranscription> {
    const startTime = Date.now();

    try {
      logger.info('Starting Whisper transcription', {
        audioPath,
        userId,
      });

      // æª¢æŸ¥æª”æ¡ˆå­˜åœ¨
      if (!fs.existsSync(audioPath)) {
        throw new Error(`Audio file not found: ${audioPath}`);
      }

      // å–å¾—æª”æ¡ˆå¤§å° (ç”¨æ–¼æˆæœ¬è¨ˆç®—)
      const stats = fs.statSync(audioPath);
      const fileSizeMB = stats.size / (1024 * 1024);

      // å‘¼å« Whisper API
      const response = await this.client.audio.transcriptions.create({
        file: fs.createReadStream(audioPath),
        model: 'whisper-1',
        language: 'zh', // ç¹é«”ä¸­æ–‡
        response_format: 'verbose_json', // å–å¾—è©³ç´°è³‡è¨ŠåŒ…å«æ™‚é–“ç¢¼
        timestamp_granularities: ['word'], // å­—ç´šæ™‚é–“ç¢¼
      });

      const duration = Date.now() - startTime;

      // è¿½è¹¤æˆæœ¬
      await this.trackCost(userId, fileSizeMB, duration);

      logger.info('Whisper transcription completed', {
        duration,
        textLength: response.text.length,
        wordCount: response.words?.length || 0,
      });

      // è§£æå›æ‡‰
      return {
        text: response.text,
        language: response.language || 'zh',
        duration: response.duration || 0,
        words: response.words || [],
      };

    } catch (error) {
      logger.error('Whisper transcription failed', {
        error: error.message,
        audioPath,
      });
      throw new Error(`Whisper transcription failed: ${error.message}`);
    }
  }

  /**
   * è¿½è¹¤ Whisper API ä½¿ç”¨æˆæœ¬
   */
  private async trackCost(
    userId: string,
    fileSizeMB: number,
    duration: number
  ): Promise<void> {
    // Whisper è¨ˆåƒ¹: $0.006 / åˆ†é˜
    // å‡è¨­éŸ³è¨Šé•·åº¦ç´„ç­‰æ–¼æª”æ¡ˆå¤§å° MB æ•¸ (ç²—ç•¥ä¼°ç®—)
    const estimatedMinutes = fileSizeMB / 1; // 1MB â‰ˆ 1 åˆ†é˜
    const cost = estimatedMinutes * 0.006;

    await this.costTracker.track({
      userId,
      service: 'whisper-stt',
      operation: 'transcribe',
      inputTokens: 0,
      outputTokens: 0,
      cost,
      duration,
      metadata: {
        fileSizeMB,
        estimatedMinutes,
      },
    });
  }
}

/**
 * Whisper è½‰éŒ„çµæœ
 */
export interface WhisperTranscription {
  text: string;
  language: string;
  duration: number;
  words: WhisperWord[];
}

/**
 * Whisper å­—ç´šæ™‚é–“ç¢¼
 */
export interface WhisperWord {
  word: string;
  start: number;
  end: number;
}
```

---

### æ­¥é©Ÿ 3: å»ºç«‹é…éŸ³åˆ‡åˆ†å¼•æ“

å»ºç«‹ `backend/src/engines/voiceover-splitter.ts`:

```typescript
/**
 * é…éŸ³åˆ‡åˆ†å¼•æ“
 *
 * è² è²¬å°‡é…éŸ³æª”æ¡ˆæŒ‰ç…§æ™‚é–“ç¢¼åˆ‡åˆ†æˆå¤šå€‹ç‰‡æ®µ
 */

import ffmpeg from 'fluent-ffmpeg';
import path from 'path';
import fs from 'fs/promises';
import { WhisperService, WhisperWord } from '../services/whisper.service';
import { supabase } from '../lib/supabase';
import { logger } from '../lib/logger';

export class VoiceoverSplitter {
  private whisper: WhisperService;

  constructor() {
    this.whisper = new WhisperService();
  }

  /**
   * è™•ç†é…éŸ³æª”æ¡ˆ: è½‰éŒ„ + åˆ‡åˆ†
   *
   * @param audioPath - æœ¬åœ°éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
   * @param userId - ä½¿ç”¨è€… ID
   * @returns åˆ‡åˆ†çµæœ
   */
  async process(
    audioPath: string,
    userId: string
  ): Promise<VoiceoverProcessResult> {
    logger.info('Starting voiceover processing', { audioPath, userId });

    try {
      // 1. ä½¿ç”¨ Whisper è½‰éŒ„
      const transcription = await this.whisper.transcribe(audioPath, userId);

      // 2. å°‡å­—çµ„åˆæˆå¥å­ç‰‡æ®µ
      const segments = this.groupWordsIntoSegments(transcription.words);

      logger.info('Grouped words into segments', {
        wordCount: transcription.words.length,
        segmentCount: segments.length,
      });

      // 3. åˆ‡åˆ†éŸ³è¨Šæª”æ¡ˆ
      const splitResults: SplitSegment[] = [];

      for (let i = 0; i < segments.length; i++) {
        const segment = segments[i];

        try {
          const outputPath = await this.splitAudio(
            audioPath,
            segment.start,
            segment.end,
            i
          );

          splitResults.push({
            text: segment.text,
            start: segment.start,
            end: segment.end,
            duration: segment.end - segment.start,
            localPath: outputPath,
          });

          logger.info('Audio segment split', {
            index: i,
            text: segment.text.substring(0, 30),
            duration: segment.end - segment.start,
          });

        } catch (error) {
          logger.error('Failed to split audio segment', {
            index: i,
            error: error.message,
          });
        }
      }

      // 4. ä¸Šå‚³åˆ° Supabase Storage
      const uploadedSegments: VoiceoverSegment[] = [];

      for (const segment of splitResults) {
        try {
          const audioUrl = await this.uploadSegment(
            segment.localPath,
            userId
          );

          uploadedSegments.push({
            ...segment,
            audioUrl,
          });

          // åˆªé™¤æœ¬åœ°æª”æ¡ˆ
          await fs.unlink(segment.localPath);

        } catch (error) {
          logger.error('Failed to upload segment', {
            text: segment.text,
            error: error.message,
          });
        }
      }

      // 5. å„²å­˜åˆ°è³‡æ–™åº«
      await this.saveSegments(uploadedSegments, audioPath, userId);

      logger.info('Voiceover processing completed', {
        totalSegments: uploadedSegments.length,
      });

      return {
        totalSegments: uploadedSegments.length,
        segments: uploadedSegments,
      };

    } catch (error) {
      logger.error('Voiceover processing failed', {
        error: error.message,
        audioPath,
      });
      throw error;
    }
  }

  /**
   * å°‡å–®å­—çµ„åˆæˆå¥å­ç‰‡æ®µ
   *
   * ç­–ç•¥: ä»¥æ¨™é»ç¬¦è™Ÿæˆ–æ™‚é–“é–“éš”åˆ†æ®µ
   */
  private groupWordsIntoSegments(words: WhisperWord[]): TextSegment[] {
    const segments: TextSegment[] = [];
    let currentSegment: WhisperWord[] = [];
    let currentText = '';

    for (let i = 0; i < words.length; i++) {
      const word = words[i];
      currentSegment.push(word);
      currentText += word.word;

      // åˆ¤æ–·æ˜¯å¦æ‡‰è©²çµæŸç•¶å‰ç‰‡æ®µ
      const shouldSplit =
        // æœ‰æ¨™é»ç¬¦è™Ÿ
        /[ã€‚!?ã€,]$/.test(word.word) ||
        // æˆ–è€…ä¸‹ä¸€å€‹å­—çš„é–“éš”è¶…é 1 ç§’
        (i < words.length - 1 && words[i + 1].start - word.end > 1.0) ||
        // æˆ–è€…æ˜¯æœ€å¾Œä¸€å€‹å­—
        i === words.length - 1;

      if (shouldSplit && currentSegment.length > 0) {
        segments.push({
          text: currentText.trim(),
          start: currentSegment[0].start,
          end: currentSegment[currentSegment.length - 1].end,
        });

        currentSegment = [];
        currentText = '';
      }
    }

    return segments;
  }

  /**
   * ä½¿ç”¨ FFmpeg åˆ‡åˆ†éŸ³è¨Š
   *
   * @param inputPath - è¼¸å…¥éŸ³è¨Šæª”æ¡ˆ
   * @param startTime - é–‹å§‹æ™‚é–“ (ç§’)
   * @param endTime - çµæŸæ™‚é–“ (ç§’)
   * @param index - ç‰‡æ®µç´¢å¼•
   * @returns è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
   */
  private async splitAudio(
    inputPath: string,
    startTime: number,
    endTime: number,
    index: number
  ): Promise<string> {
    return new Promise((resolve, reject) => {
      const outputDir = path.join(process.cwd(), 'temp', 'voiceover-segments');
      const outputPath = path.join(
        outputDir,
        `segment_${index}_${Date.now()}.mp3`
      );

      // ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
      fs.mkdir(outputDir, { recursive: true }).catch(() => {});

      // è½‰æ›æ™‚é–“æ ¼å¼
      const startTimeStr = this.secondsToFFmpegTime(startTime);
      const endTimeStr = this.secondsToFFmpegTime(endTime);

      logger.info('Splitting audio', {
        inputPath,
        startTime: startTimeStr,
        endTime: endTimeStr,
        outputPath,
      });

      ffmpeg(inputPath)
        .setStartTime(startTimeStr)
        .setDuration(endTime - startTime)
        .audioCodec('copy') // ä¸é‡æ–°ç·¨ç¢¼,ä¿æŒå“è³ª
        .output(outputPath)
        .on('end', () => {
          logger.info('Audio split completed', { outputPath });
          resolve(outputPath);
        })
        .on('error', (error) => {
          logger.error('Audio split failed', {
            error: error.message,
            outputPath,
          });
          reject(error);
        })
        .run();
    });
  }

  /**
   * å°‡ç§’æ•¸è½‰æ›ç‚º FFmpeg æ™‚é–“æ ¼å¼
   */
  private secondsToFFmpegTime(seconds: number): string {
    const hours = Math.floor(seconds / 3600);
    const minutes = Math.floor((seconds % 3600) / 60);
    const secs = seconds % 60;

    return `${hours.toString().padStart(2, '0')}:${minutes
      .toString()
      .padStart(2, '0')}:${secs.toFixed(3).padStart(6, '0')}`;
  }

  /**
   * ä¸Šå‚³ç‰‡æ®µåˆ° Supabase Storage
   */
  private async uploadSegment(
    localPath: string,
    userId: string
  ): Promise<string> {
    const fileName = path.basename(localPath);
    const storagePath = `voiceover-segments/${userId}/${fileName}`;

    const fileBuffer = await fs.readFile(localPath);

    const { data, error } = await supabase.storage
      .from('audio')
      .upload(storagePath, fileBuffer, {
        contentType: 'audio/mpeg',
        upsert: false,
      });

    if (error) {
      throw new Error(`Failed to upload segment: ${error.message}`);
    }

    // å–å¾—å…¬é–‹ URL
    const { data: urlData } = supabase.storage
      .from('audio')
      .getPublicUrl(storagePath);

    return urlData.publicUrl;
  }

  /**
   * å„²å­˜ç‰‡æ®µåˆ°è³‡æ–™åº«
   */
  private async saveSegments(
    segments: VoiceoverSegment[],
    originalAudioUrl: string,
    userId: string
  ): Promise<void> {
    const records = segments.map((segment) => ({
      user_id: userId,
      original_audio_url: originalAudioUrl,
      text: segment.text,
      start_time: segment.start,
      end_time: segment.end,
      duration: segment.duration,
      segment_audio_url: segment.audioUrl,
    }));

    const { error } = await supabase
      .from('voiceover_segments')
      .insert(records);

    if (error) {
      throw new Error(`Failed to save segments: ${error.message}`);
    }

    logger.info('Saved segments to database', {
      count: segments.length,
    });
  }
}

/**
 * æ–‡å­—ç‰‡æ®µ
 */
interface TextSegment {
  text: string;
  start: number;
  end: number;
}

/**
 * åˆ‡åˆ†å¾Œçš„ç‰‡æ®µ
 */
interface SplitSegment {
  text: string;
  start: number;
  end: number;
  duration: number;
  localPath: string;
}

/**
 * é…éŸ³ç‰‡æ®µ
 */
interface VoiceoverSegment extends SplitSegment {
  audioUrl: string;
}

/**
 * è™•ç†çµæœ
 */
export interface VoiceoverProcessResult {
  totalSegments: number;
  segments: VoiceoverSegment[];
}
```

---

### æ­¥é©Ÿ 4: å»ºç«‹ API ç«¯é»

åœ¨ `backend/src/routes/voiceover.ts` å»ºç«‹ç«¯é»:

```typescript
import { Router } from 'express';
import multer from 'multer';
import path from 'path';
import { VoiceoverSplitter } from '../engines/voiceover-splitter';
import { authenticate } from '../middleware/auth';

const router = Router();
const splitter = new VoiceoverSplitter();

// è¨­å®šæª”æ¡ˆä¸Šå‚³
const upload = multer({
  dest: 'temp/uploads/',
  limits: {
    fileSize: 50 * 1024 * 1024, // 50MB
  },
  fileFilter: (req, file, cb) => {
    const allowedTypes = ['.mp3', '.wav', '.m4a', '.mp4'];
    const ext = path.extname(file.originalname).toLowerCase();

    if (allowedTypes.includes(ext)) {
      cb(null, true);
    } else {
      cb(new Error('Invalid file type. Only audio files are allowed.'));
    }
  },
});

/**
 * POST /api/voiceover/upload
 *
 * ä¸Šå‚³é…éŸ³æª”æ¡ˆä¸¦è‡ªå‹•åˆ‡åˆ†
 */
router.post('/upload', authenticate, upload.single('audio'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({
        success: false,
        error: 'No audio file uploaded',
      });
    }

    const userId = req.user.id;
    const audioPath = req.file.path;

    // è™•ç†é…éŸ³æª”æ¡ˆ
    const result = await splitter.process(audioPath, userId);

    res.json({
      success: true,
      result,
    });

  } catch (error) {
    res.status(500).json({
      success: false,
      error: error.message,
    });
  }
});

export default router;
```

---

### æ­¥é©Ÿ 5: è¨»å†Šè·¯ç”±

åœ¨ `backend/src/index.ts` åŠ å…¥è·¯ç”±:

```typescript
import voiceoverRoutes from './routes/voiceover';

// ...

app.use('/api/voiceover', voiceoverRoutes);
```

---

### æ­¥é©Ÿ 6: æ¸¬è©¦é…éŸ³åˆ‡åˆ†

```bash
# å•Ÿå‹•é–‹ç™¼ä¼ºæœå™¨
npm run dev

# ä½¿ç”¨ curl æ¸¬è©¦ä¸Šå‚³
curl -X POST http://localhost:8080/api/voiceover/upload \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "audio=@test-voiceover.mp3"
```

**é æœŸçµæœ**:
```json
{
  "success": true,
  "result": {
    "totalSegments": 15,
    "segments": [
      {
        "text": "å¤§å®¶å¥½,ä»Šå¤©è¦ä»‹ç´¹ AI å½±ç‰‡å‰ªè¼¯ã€‚",
        "start": 0.0,
        "end": 2.5,
        "duration": 2.5,
        "audioUrl": "https://..."
      }
    ]
  }
}
```

---

## âœ… é©—æ”¶æ¨™æº–

å®Œæˆæ‰€æœ‰å¯¦ä½œæ­¥é©Ÿå¾Œ,åŸ·è¡Œé©—æ”¶æ¸¬è©¦ç¢ºèªä¸€åˆ‡æ­£å¸¸ã€‚

### é©—æ”¶æ¸¬è©¦æ¶æ§‹

æœ¬ Task åŒ…å«ä¸‰å±¤é©—æ”¶æ¸¬è©¦:

- ğŸ“ **Basic Verification** (5 tests): åŸºç¤åŠŸèƒ½é©—è­‰
- ğŸ“ **Functional Acceptance** (6 tests): åŠŸèƒ½æ­£ç¢ºæ€§
- ğŸ“ **E2E Acceptance** (3 tests): å®Œæ•´æµç¨‹

### åŸ·è¡Œé©—æ”¶

```bash
# ä¸€éµåŸ·è¡Œæ‰€æœ‰é©—æ”¶æ¸¬è©¦
npm run verify:task task-2.7

# æˆ–åˆ†åˆ¥åŸ·è¡Œ
npm test -- tests/phase-2/task-2.7.basic.test.ts
npm test -- tests/phase-2/task-2.7.functional.test.ts
npm test -- tests/phase-2/task-2.7.e2e.test.ts
```

### é€šéæ¨™æº–

- âœ… æ‰€æœ‰ 14 å€‹æ¸¬è©¦é€šé (5 + 6 + 3)
- âœ… Whisper API å¯ä»¥æ­£å¸¸è½‰éŒ„
- âœ… FFmpeg å¯ä»¥æ­£ç¢ºåˆ‡åˆ†éŸ³è¨Š
- âœ… ç‰‡æ®µæ­£ç¢ºå„²å­˜åˆ°è³‡æ–™åº«

<details>
<summary>ğŸ“Š æŸ¥çœ‹è©³ç´°æ¸¬è©¦é …ç›®æ¸…å–®</summary>

### Basic Verification (5 tests)

æ¸¬è©¦æª”æ¡ˆ: `tests/phase-2/task-2.7.basic.test.ts`

1. âœ“ Whisper API å¯ä»¥é€£æ¥
2. âœ“ FFmpeg å·²æ­£ç¢ºå®‰è£
3. âœ“ å¯ä»¥è½‰éŒ„ç°¡å–®éŸ³è¨Š
4. âœ“ å¯ä»¥åˆ‡åˆ†éŸ³è¨Šæª”æ¡ˆ
5. âœ“ æ™‚é–“ç¢¼æ ¼å¼è½‰æ›æ­£ç¢º

### Functional Acceptance (6 tests)

æ¸¬è©¦æª”æ¡ˆ: `tests/phase-2/task-2.7.functional.test.ts`

1. âœ“ æ­£ç¢ºå–å¾—å­—ç´šæ™‚é–“ç¢¼
2. âœ“ æ­£ç¢ºçµ„åˆæˆå¥å­ç‰‡æ®µ
3. âœ“ éŸ³è¨Šåˆ‡åˆ†å“è³ªè‰¯å¥½
4. âœ“ ç‰‡æ®µæ­£ç¢ºä¸Šå‚³åˆ° Storage
5. âœ“ ç‰‡æ®µæ­£ç¢ºå„²å­˜åˆ°è³‡æ–™åº«
6. âœ“ æˆæœ¬è¿½è¹¤æ­£ç¢ºè¨˜éŒ„

### E2E Acceptance (3 tests)

æ¸¬è©¦æª”æ¡ˆ: `tests/phase-2/task-2.7.e2e.test.ts`

1. âœ“ å®Œæ•´é…éŸ³è™•ç†æµç¨‹æˆåŠŸ
2. âœ“ æ‰¹æ¬¡è™•ç†å¤šå€‹é…éŸ³æª”
3. âœ“ éŒ¯èª¤è™•ç†æ­£ç¢º

</details>

---

## ğŸ“‹ å®Œæˆæª¢æŸ¥æ¸…å–®

å®Œæˆé€™å€‹ Task å¾Œ,è«‹ç¢ºèªä»¥ä¸‹é …ç›®:

### API è¨­å®š
- [ ] OpenAI API Key å·²è¨­å®š
- [ ] Whisper API é€£æ¥æ¸¬è©¦é€šé
- [ ] FFmpeg å·²å®‰è£ä¸¦å¯åŸ·è¡Œ

### æ ¸å¿ƒå¯¦ä½œ
- [ ] `WhisperService` å·²å»ºç«‹
- [ ] `VoiceoverSplitter` å·²å»ºç«‹
- [ ] API ç«¯é»å·²å»ºç«‹
- [ ] è·¯ç”±å·²è¨»å†Š

### è³‡æ–™åº«
- [ ] `voiceover_segments` è¡¨å·²å»ºç«‹
- [ ] ç‰‡æ®µå¯ä»¥æ­£ç¢ºå„²å­˜
- [ ] å¯ä»¥æŸ¥è©¢é…éŸ³ç‰‡æ®µ

### åŠŸèƒ½é©—è­‰
- [ ] å¯ä»¥ä¸Šå‚³é…éŸ³æª”æ¡ˆ
- [ ] å¯ä»¥è‡ªå‹•è½‰éŒ„æ–‡å­—
- [ ] å¯ä»¥è‡ªå‹•åˆ‡åˆ†ç‰‡æ®µ
- [ ] å¯ä»¥æ­£ç¢ºå„²å­˜ç‰‡æ®µ
- [ ] éŸ³è¨Šå“è³ªè‰¯å¥½

### æ¸¬è©¦é©—æ”¶
- [ ] Basic Verification æ¸¬è©¦é€šé (5/5)
- [ ] Functional Acceptance æ¸¬è©¦é€šé (6/6)
- [ ] E2E Acceptance æ¸¬è©¦é€šé (3/3)
- [ ] **ç¸½è¨ˆ: 14/14 æ¸¬è©¦é€šé**

---

## ğŸ“Š Logging èˆ‡éŒ¯èª¤è™•ç†æ•´åˆ

> åƒè€ƒ: [LOGGING-STANDARDS.md](../LOGGING-STANDARDS.md)

### å¿…é ˆè¨˜éŒ„çš„äº‹ä»¶

#### åŸºç¤äº‹ä»¶
- [ ] `task_started` - ä»»å‹™é–‹å§‹
- [ ] `task_step_started` - é–‹å§‹é…éŸ³åˆ‡åˆ†
- [ ] `task_step_completed` - åˆ‡åˆ†å®Œæˆ
- [ ] `task_completed` - ä»»å‹™å®Œæˆ
- [ ] `task_failed` - ä»»å‹™å¤±æ•—

#### AI å‘¼å«äº‹ä»¶ (é€é PromptManager)
- [ ] `ai_call_started`, `ai_call_completed`, `ai_call_failed`
- [ ] `ai_response_validation_failed` - Schema æˆ–æ™‚é–“è»¸é©—è­‰å¤±æ•—

### æ•´åˆç¨‹å¼ç¢¼ç¯„ä¾‹

```typescript
import { validateVoiceoverTiming } from '@/services/validators/data-flow.validator'

class VoiceoverSplitEngine {
  async split(voiceoverId: string, userId: string) {
    const taskLogger = createTaskLogger('voiceover_split', userId)
    const executionId = taskLogger.getExecutionId()
    const validator = new DataFlowValidator(taskLogger.getLogger())
    const callId = uuid()

    try {
      const voiceover = await db.voiceovers.findOne({ voiceoverId })

      await taskLogger.taskStarted(
        {
          voiceoverId,
          transcript_length: voiceover.transcript.length,
          duration: voiceover.duration
        },
        ['ai_split', 'validate_timing', 'save_segments']
      )

      // Step 1: AI åˆ‡åˆ†
      await taskLogger.stepStarted(0, 'ai_split')

      const { response, cost } = await promptManager.executePrompt(
        'voiceover-processing',
        'voiceover-split',
        {
          transcript: voiceover.transcript,
          duration: voiceover.duration
        },
        { executionId, userId, callId }
      )

      // âœ… é©—è­‰ Schema
      await validator.validateAIResponse(
        callId,
        'voiceover_split',  // åœ¨ schemas.ts ä¸­å·²å®šç¾©
        response
      )

      await taskLogger.stepCompleted(0, 'ai_split', {
        segments_count: response.segments.length
      })

      // Step 2: é©—è­‰æ™‚é–“è»¸ä¸€è‡´æ€§ âœ… é—œéµé©—è­‰ï¼
      await taskLogger.stepStarted(1, 'validate_timing')

      await validateVoiceoverTiming(
        response.segments,
        voiceover.duration,
        taskLogger.getLogger()
      )

      await taskLogger.stepCompleted(1, 'validate_timing')

      // Step 3: å„²å­˜ç‰‡æ®µ
      await taskLogger.stepStarted(2, 'save_segments')
      // ... å„²å­˜é‚è¼¯ ...
      await taskLogger.stepCompleted(2, 'save_segments')

      await taskLogger.taskCompleted(
        { segments_created: response.segments.length },
        cost
      )

      return response.segments

    } catch (error) {
      await taskLogger.taskFailed('ai_split', error, {
        voiceoverId,
        duration: voiceover?.duration
      })
      throw error  // âœ… Fail Fast
    }
  }
}
```

### å¿…é ˆé©—è­‰çš„è³‡æ–™

æ ¹æ“š `schemas.ts`:
- [x] `segments`: array (min 1)
- [x] æ¯å€‹ segment: `{ start: number, end: number, text: string, keywords: string[] }`

**æ™‚é–“è»¸ä¸€è‡´æ€§** (ä½¿ç”¨ `validateVoiceoverTiming`):
- [x] start < end
- [x] ç„¡ç¸«éš™ (segment[i].end == segment[i+1].start)
- [x] ç„¡é‡ç–Š
- [x] ä¸è¶…éç¸½é•·åº¦

### Fail Fast æª¢æŸ¥æ¸…å–®

- [x] âœ… Schema é©—è­‰å¤±æ•—æ™‚ç«‹å³ throw error
- [x] âœ… æ™‚é–“è»¸é©—è­‰å¤±æ•—æ™‚ç«‹å³ throw error (ä¸å˜—è©¦ä¿®å¾©)
- [x] âœ… è¨˜éŒ„è©³ç´°çš„æ™‚é–“è»¸éŒ¯èª¤ (å“ªå€‹ segment æœ‰å•é¡Œ)

---

## ğŸ› å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å¸¸è¦‹éŒ¯èª¤é¡å‹é€ŸæŸ¥è¡¨

| éŒ¯èª¤è¨Šæ¯ | å¯èƒ½åŸå›  | å¿«é€Ÿè§£æ³• |
|---------|---------|---------|
| `FFmpeg not found` | FFmpeg æœªå®‰è£ | å®‰è£ FFmpeg |
| `Audio codec not supported` | éŸ³è¨Šæ ¼å¼å•é¡Œ | è½‰æ›ç‚º mp3 æ ¼å¼ |
| `Whisper API timeout` | æª”æ¡ˆå¤ªå¤§ | å£“ç¸®éŸ³è¨Šæˆ–åˆ†æ®µè™•ç† |
| `Invalid timestamp` | æ™‚é–“ç¢¼æ ¼å¼éŒ¯èª¤ | æª¢æŸ¥æ™‚é–“ç¢¼è½‰æ›å‡½å¼ |
| `Segment duration too short` | åˆ‡åˆ†é»å¤ªå¯†é›† | èª¿æ•´åˆ†æ®µç­–ç•¥ |

---

### å•é¡Œ 1: FFmpeg æœªå®‰è£æˆ–æ‰¾ä¸åˆ°

**éŒ¯èª¤è¨Šæ¯:**
```
Error: spawn ffmpeg ENOENT
```

**è§£æ±ºæ–¹æ¡ˆ:**

```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt-get install ffmpeg

# Windows (ä½¿ç”¨ Chocolatey)
choco install ffmpeg

# é©—è­‰å®‰è£
ffmpeg -version
```

---

### å•é¡Œ 2: Whisper è½‰éŒ„å¤±æ•—

**éŒ¯èª¤è¨Šæ¯:**
```
Error: Request timed out
```

**è§£æ±ºæ–¹æ¡ˆ:**

éŸ³è¨Šæª”æ¡ˆå¤ªå¤§æ™‚,Whisper å¯èƒ½è¶…æ™‚ã€‚è§£æ±ºæ–¹æ³•:

```typescript
// 1. åœ¨ä¸Šå‚³å‰å£“ç¸®éŸ³è¨Š
private async compressAudio(inputPath: string): Promise<string> {
  const outputPath = inputPath.replace('.mp3', '_compressed.mp3');

  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .audioBitrate('64k') // é™ä½ä½å…ƒç‡
      .audioChannels(1) // å–®è²é“
      .output(outputPath)
      .on('end', () => resolve(outputPath))
      .on('error', reject)
      .run();
  });
}

// 2. åˆ†æ®µè™•ç†é•·éŸ³è¨Š
async transcribeLongAudio(audioPath: string): Promise<WhisperTranscription[]> {
  const segments = await this.splitLongAudio(audioPath, 600); // æ¯ 10 åˆ†é˜åˆ‡ä¸€æ®µ
  const results = [];

  for (const segment of segments) {
    const result = await this.whisper.transcribe(segment);
    results.push(result);
  }

  return results;
}
```

---

### å•é¡Œ 3: éŸ³è¨Šåˆ‡åˆ†å“è³ªä¸‹é™

**å•é¡Œ**: åˆ‡åˆ†å¾Œçš„éŸ³è¨Šæœ‰é›œéŸ³æˆ–å“è³ªè®Šå·®

**è§£æ±ºæ–¹æ¡ˆ:**

ç¢ºä¿ä½¿ç”¨ `-c copy` åƒæ•¸:

```typescript
ffmpeg(inputPath)
  .setStartTime(startTimeStr)
  .setDuration(duration)
  .audioCodec('copy') // â† é‡è¦: ä¸é‡æ–°ç·¨ç¢¼
  .output(outputPath)
  .run();
```

å¦‚æœé‚„æ˜¯æœ‰å•é¡Œ,æª¢æŸ¥åŸå§‹æª”æ¡ˆæ ¼å¼:

```bash
# æª¢æŸ¥éŸ³è¨Šè³‡è¨Š
ffprobe input.mp3

# å¦‚æœæ ¼å¼ä¸æ”¯æ´ copy,å…ˆè½‰æ›æ ¼å¼
ffmpeg -i input.wav -c:a libmp3lame -q:a 2 input.mp3
```

---

### å•é¡Œ 4: å¥å­åˆ‡åˆ†ä¸æ­£ç¢º

**å•é¡Œ**: å¥å­åˆ‡å¾—å¤ªçŸ­æˆ–å¤ªé•·

**è§£æ±ºæ–¹æ¡ˆ:**

èª¿æ•´åˆ†æ®µé‚è¼¯:

```typescript
private groupWordsIntoSegments(words: WhisperWord[]): TextSegment[] {
  const segments: TextSegment[] = [];
  let currentSegment: WhisperWord[] = [];
  let currentText = '';

  const MAX_SEGMENT_DURATION = 10; // æœ€é•· 10 ç§’
  const MIN_SEGMENT_DURATION = 2;  // æœ€çŸ­ 2 ç§’

  for (let i = 0; i < words.length; i++) {
    const word = words[i];
    currentSegment.push(word);
    currentText += word.word;

    const currentDuration = word.end - currentSegment[0].start;

    const shouldSplit =
      // æœ‰æ¨™é»ç¬¦è™Ÿä¸”é•·åº¦è¶³å¤ 
      (/[ã€‚!?]$/.test(word.word) && currentDuration >= MIN_SEGMENT_DURATION) ||
      // æˆ–è€…è¶…éæœ€å¤§é•·åº¦
      currentDuration >= MAX_SEGMENT_DURATION ||
      // æˆ–è€…æ˜¯æœ€å¾Œä¸€å€‹å­—
      i === words.length - 1;

    if (shouldSplit && currentSegment.length > 0) {
      segments.push({
        text: currentText.trim(),
        start: currentSegment[0].start,
        end: word.end,
      });

      currentSegment = [];
      currentText = '';
    }
  }

  return segments;
}
```

---

### å•é¡Œ 5: ç¹é«”ä¸­æ–‡è¾¨è­˜ä¸æº–ç¢º

**å•é¡Œ**: Whisper å°‡ç¹é«”ä¸­æ–‡è½‰æˆç°¡é«”ä¸­æ–‡

**è§£æ±ºæ–¹æ¡ˆ:**

Whisper çš„ `language` åƒæ•¸è¨­ç‚º `zh` æœƒæ··åˆç¹ç°¡é«”ã€‚å¯ä»¥:

```typescript
// æ–¹æ¡ˆ 1: å¾Œè™•ç†è½‰æ›
import { convert } from 'opencc';

async transcribe(audioPath: string): Promise<WhisperTranscription> {
  const response = await this.client.audio.transcriptions.create({
    file: fs.createReadStream(audioPath),
    model: 'whisper-1',
    language: 'zh',
    response_format: 'verbose_json',
    timestamp_granularities: ['word'],
  });

  // è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
  const converter = new convert({ from: 'cn', to: 'tw' });
  const traditionalText = converter(response.text);

  return {
    text: traditionalText,
    // ...
  };
}

// æ–¹æ¡ˆ 2: ä½¿ç”¨ prompt æç¤º
async transcribe(audioPath: string): Promise<WhisperTranscription> {
  const response = await this.client.audio.transcriptions.create({
    file: fs.createReadStream(audioPath),
    model: 'whisper-1',
    language: 'zh',
    response_format: 'verbose_json',
    timestamp_granularities: ['word'],
    prompt: 'è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚', // æç¤ºä½¿ç”¨ç¹é«”
  });

  return response;
}
```

---

## ğŸ“š å»¶ä¼¸å­¸ç¿’è³‡æº

å¦‚æœä½ æƒ³æ·±å…¥äº†è§£é€™å€‹ Task ä½¿ç”¨çš„æŠ€è¡“:

- **OpenAI Whisper API**: https://platform.openai.com/docs/guides/speech-to-text
- **FFmpeg æ–‡ä»¶**: https://ffmpeg.org/documentation.html
- **Audio Processing Guide**: https://trac.ffmpeg.org/wiki/AudioChannelManipulation
- **Fluent-FFmpeg**: https://github.com/fluent-ffmpeg/node-fluent-ffmpeg

---

## âœ… Task å®Œæˆç¢ºèª

ç•¶ä»¥ä¸‹æ‰€æœ‰é …ç›®éƒ½å®Œæˆæ™‚,é€™å€‹ Task å°±ç®—å®Œæˆ:

1. âœ… æ‰€æœ‰å¯¦ä½œæ­¥é©Ÿéƒ½å®Œæˆ
2. âœ… æ‰€æœ‰ä¸‰å±¤é©—æ”¶æ¸¬è©¦éƒ½é€šé (14/14)
3. âœ… å®Œæˆæª¢æŸ¥æ¸…å–®éƒ½å‹¾é¸
4. âœ… å¯ä»¥æˆåŠŸä¸Šå‚³é…éŸ³ä¸¦è‡ªå‹•åˆ‡åˆ†

### æœ€çµ‚é©—æ”¶æŒ‡ä»¤

```bash
# é€²å…¥ backend ç›®éŒ„
cd backend

# åŸ·è¡Œé©—æ”¶æ¸¬è©¦
npm run verify:task task-2.7

# å¦‚æœå…¨éƒ¨é€šé,ä½ æ‡‰è©²çœ‹åˆ°:
# PASS tests/phase-2/task-2.7.basic.test.ts
# PASS tests/phase-2/task-2.7.functional.test.ts
# PASS tests/phase-2/task-2.7.e2e.test.ts
#
# Test Suites: 3 passed, 3 total
# Tests:       14 passed, 14 total
```

**æ­å–œ!** å¦‚æœçœ‹åˆ°ä¸Šé¢çš„è¼¸å‡º,ä»£è¡¨ Task 2.7 å®Œæˆäº†!

---

## ğŸ“ å»ºè­°ç´€éŒ„

å»ºè­°åœ¨ä½ çš„ç­†è¨˜æœ¬æˆ–å°ˆæ¡ˆç®¡ç†å·¥å…·ä¸­è¨˜éŒ„:
- Task å®Œæˆæ™‚é–“
- Whisper çš„å¯¦éš›è¾¨è­˜æº–ç¢ºåº¦
- é…éŸ³åˆ‡åˆ†çš„å“è³ªå¦‚ä½•
- é‡åˆ°çš„ä¸»è¦å•é¡Œèˆ‡è§£æ±ºæ–¹æ³•
- æˆæœ¬ä¼°ç®—æ˜¯å¦æº–ç¢º

é€™äº›è¨˜éŒ„åœ¨ä¹‹å¾Œå„ªåŒ–æ™‚æœƒå¾ˆæœ‰ç”¨!

---

**ä¸‹ä¸€æ­¥**: ç¹¼çºŒ Task 2.8 - å€™é¸ç‰‡æ®µæŸ¥è©¢

---

**æ–‡ä»¶ç‰ˆæœ¬**: 2.0
**æœ€å¾Œæ›´æ–°**: 2025-10-07
**ç¶­è­·è€…**: CheapCut é–‹ç™¼åœ˜éšŠ
